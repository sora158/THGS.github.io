<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="THGS: 3D Talking Human Avatar Synthesis via Gaussian Splatting.">
    <title>THGS: 3D Talking Human Avatar Synthesis via Gaussian Splatting</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.3/css/bulma.min.css">
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        .centered-content {
            text-align: center;
        }
        .centered-image, .dynamic-image, .dynamic-video {
            display: block;
            margin: 20px auto;
            max-width: 100%;
        }
        .video-description {
            text-align: center;
            margin-top: 10px;
            font-size: 1.1em;
        }
        .content p {
            text-align: justify;
        }
        .sub-description {
            text-align: center;
            font-size: 0.9em;
        }
        .index-link {
            font-size: 1.1em;
            margin-bottom: 15px;
            display: block;
        }
        .github-link {
            font-size: 1.2em;
            margin-top: 20px;
            display: block;
            text-align: center;
        }
    </style>
</head>
<body>
    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-four-fifths centered-content">
                    <h1 class="title is-1">THGS: 3D Talking Human Avatar Synthesis via Gaussian Splatting</h1>
                    <a href="https://sora158.github.io/THGS.github.io/" class="github-link" target="_blank">GitHub: Code to be continued</a>
                    <div class="content">
                        <h2 class="title is-3">Video Index</h3>
                        <a href="#video1" class="index-link">3D Video</a>
                        <a href="#video2" class="index-link">Self Reenactment</a>
                        <a href="#video3" class="index-link">Expression Control</a>
                        <a href="#video4" class="index-link">Joint Control</a>
                    </div>
                    <img src="control.gif" alt="explicit control results" class="centered-image">    
                    <sub class="sub-description">
                        Learning from a one-minute monocular video, THGS can learn a 3DGS-based talking human avatar,<br> and we can explicitly control over expression, body joints and camera pose.
                    </sub>
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content">
                        <p>
                            Despite the remarkable progress in NeRF/3DGS-based talking head solutions, 
                            directly generating 3D talking human avatars remains challenging. We propose <em>THGS</em>,
                            a novel method that extends talking head techniques to reconstruct expressive human avatars using 3D Gaussian Splatting (3DGS) from dynamic monocular videos.
                            After training on videos, <em>THGS</em> can animate speaker-specific human avatars with facial dynamics and hand gestures given speech audio and SMPL-X pose sequences.
                        </p>
                        <p>
                            <em>THGS</em> effectively overcomes the limitations of 3DGS-based human reconstruction methods in capturing expressive features like <strong>mouth movements, 
                            facial expressions, and hand gestures</strong> from dynamic monocular videos.
                        </p>
                        <p>
                            The key contributions of this paper are threefold. Firstly, we introduce a simple yet effective learnable expression blendshapes for head dynamics reconstruction, where avatar expression can be generated by linearly combining the static head model and expression blendshapes. 
                            Secondly, we employ a pose and expression coefficient refinement technique to optimize hand pose and facial expression accuracy on the fly by aligning 3D Gaussians with a human template mesh, which is crucial for human reconstruction pipeline. Thirdly, we use a <strong>Spatial Audio Attention Module (SAAM)</strong> for lip-synced mouth movement animation, which builds connections between speech audio and mouth Gaussian movements. 
                            Experimental results demonstrate that <em>THGS</em> achieves high-fidelity expressive 3D talking Gaussian human avatar animation at over 150 fps on a web-based rendering system.
                        </p>
                    </div>
                    <div id="image-container" class="content">
                        <img src="pipeline.png" class="dynamic-image" alt="Pipeline Image">
                        <p class="sub-description"><strong>Our Pipeline.</strong> Taking a monocular video as input, we learn a 3D Gaussian representation of talking human avatars. We initialize the 3D Gaussians on the SMPL-X vertices in the canonical space and drive the 3D Gaussians through LBS deformation to obtain posed Gaussians. For the LBS deformation module, we use pose θ and expression ψ as inputs. During training, θ, ψ, and skinning weights ω are optimized to achieve better body poses alignment. We also introduce a Learnable Expression Blendshapes to optimize facial dynamics reconstruction. Audio features are processed through the Spatial Audio Attention Module (SAAM), predicting mouth Gaussians deformation. We combine mouth Gaussians deformation with the posed Gaussians to produce the final Gaussians, P = {u+∆u,r+∆r,s+∆s,η,f}. Finally, a 3DGS rasterizer renders images based on camera poses.</p>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <section class="section" id="video1">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-four-fifths centered-content">
                    <video controls class="dynamic-video">
                        <source src="3Dvideos.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <p class="video-description">Our 3D video is here, rendering at a web-based visualizer(viser) at 285 FPS!</p>
                </div>
            </div>
        </div>
    </section>
    <section class="section" id="video2">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-four-fifths centered-content">
                    <video controls class="dynamic-video">
                        <source src="self_reenactment.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <p class="video-description">Our self-reenactment results. Given body pose and audio as input, our method can generate lip-synced talking human avatars with facial expressions and various hand gestures, outperforming human reconstruction SoTA method.</p>
                </div>
            </div>
        </div>
    </section>
    <section class="section" id="video3">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-four-fifths centered-content">
                    <video controls class="dynamic-video">
                        <source src="experssionControl.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <p class="video-description">Our expression control results, making smiling face, pouty lips, eyebrow movement, and blinking. Thanks to our LEB module, we can explicitly control over expression by linear interpolation of learnable expression blendshapes.</p>
                </div>
            </div>
        </div>
    </section>
    <section class="section" id="video4">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-four-fifths centered-content">
                    <video controls class="dynamic-video">
                        <source src="jointControl.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <p class="video-description">Our joint control results. We can explicitly control angles of wrist, elbow, shoulder, and head by adjusting rotation angles of SMPL-X.</p>
                </div>
            </div>
        </div>
    </section>

    <script>
        // JavaScript to dynamically set the image and video width to match the text width
        window.onload = function() {
            var textElements = document.querySelectorAll('.content p');
            var imageElements = document.querySelectorAll('.dynamic-image');
            var videoElements = document.querySelectorAll('.dynamic-video');
            textElements.forEach(function(textElement) {
                var textWidth = textElement.offsetWidth;
                imageElements.forEach(function(imageElement) {
                    imageElement.style.width = textWidth + 'px';
                });
                videoElements.forEach(function(videoElement) {
                    videoElement.style.width = textWidth + 'px';
                });
            });
        };
    </script>
</body>
</html>
