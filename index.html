<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="THGS: 3D Talking Human Avatar Synthesis via Gaussian Splatting.">
    <title>THGS: 3D Talking Human Avatar Synthesis via Gaussian Splatting</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.3/css/bulma.min.css">
</head>
<body>
    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h1 class="title is-1">THGS: 3D Talking Human Avatar Synthesis via Gaussian Splatting</h1>
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            Despite the remarkable progress in NeRF/3DGS-based talking head solutions, 
                            directly generating 3D talking human avatars remains challenging. We propose <em>THGS</em>,
                            a novel method that extends talking head techniques to reconstruct expressive human avatars using 3D Gaussian Splatting (3DGS) from dynamic monocular videos.
                            After training on videos, <em>THGS</em> can animate speaker-specific human avatars with facial dynamics and hand gestures given speech audio and SMPL-X pose sequences.
                        </p>
                        <p>
                            <em>THGS</em> effectively overcomes the limitations of 3DGS-based human reconstruction methods in capturing expressive features like <strong>mouth movements, 
                            facial expressions, and hand gestures</strong> from dynamic monocular videos.
                        </p>
                        <p>
                            The key contributions of this paper are threefold. Firstly, we introduce a simple yet effective learnable expression blendshapes for head dynamics reconstruction, where avatar expression can be generated by linearly combining the static head model and expression blendshapes. 
                            Secondly, we employ a pose and expression coefficient refinement technique to optimize hand pose and facial expression accuracy on the fly by aligning 3D Gaussians with a human template mesh, which is crucial for human reconstruction pipeline. Thirdly, we use a <strong>Spatial Audio Attention Module (SAAM)</strong> for lip-synced mouth movement animation, which builds connections between speech audio and mouth Gaussian movements. 
                            Experimental results demonstrate that <em>THGS</em> achieves high-fidelity expressive 3D talking Gaussian human avatar animation at over 150 fps on a web-based rendering system.
                        </p>
                    </div>
                </div>
            </div>
            <!--/ Abstract. -->
        </div>
    </section>
</body>
</html>
