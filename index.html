<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="THGS: 3D Talking Human Avatar Synthesis via Gaussian Splatting.">
    <title>THGS: 3D Talking Human Avatar Synthesis via Gaussian Splatting</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.3/css/bulma.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" integrity="sha512-xxxxxx" crossorigin="anonymous" />
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        .centered-content {
            text-align: center;
        }
        .centered-image, .dynamic-image, .dynamic-video {
            display: block;
            margin: 20px auto;
            max-width: 100%;
        }
        .video-description {
            text-align: center;
            margin-top: 10px;
            font-size: 1.1em;
        }
        .content p {
            text-align: justify;
        }
        .sub-description {
            text-align: center;
            font-size: 0.9em;
        }
        .index-link {
            font-size: 1.1em;
            margin-bottom: 15px;
            display: block;
        }
        .github-link {
            font-size: 1.2em;
            margin-top: 20px;
            display: block;
            text-align: center;
        }
        .content a {
            display: inline-block; /* 或者使用 display: inline; */
            margin-right: 10px; /* 可以根据需要调整链接之间的间距 */
            text-decoration: none; /* 去掉链接的下划线 */
        }
        .video-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin-top: 20px;
        }
        .dynamic-video {
            width: 100%;
            height: auto;
        }
    </style>
</head>
<body>
    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-four-fifths centered-content">
                    <h1 class="title is-1">THGS: 3D Talking Human Avatar Synthesis via Gaussian Splatting</h1>
                    <div class="content">
                        <h2 class="title is-3">Video Index</h2>
                        <a href="https://sora158.github.io/THGS.github.io/" class="github-link" target="_blank"><i class="fab fa-github"></i> Code(On the way) </a>
                        <a href="#video1" class="index-link">3D Video</a>
                        <a href="#video2" class="index-link">Self Reenactment</a>
                        <a href="#video3" class="index-link">Expression Control</a>
                        <a href="#video4" class="index-link">Joint Control</a>
                        <a href="#video5" class="index-link">Comparison of w/o LEB and w/ LEB</a>
                        <a href="#video6" class="index-link">Comparison with 3D Talking Head Methods</a>
                    </div>
                    <img src="control.gif" alt="explicit control results" class="centered-image">    
                    <sub class="sub-description">
                        Learning from a one-minute monocular video, THGS can learn a 3DGS-based talking human avatar,<br> and we can explicitly control over expression, body joints and camera pose.
                    </sub>
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content">
                        <p>
                            Despite the remarkable progress in NeRF/3DGS-based talking head solutions, 
                            directly generating 3D talking human avatars remains challenging. We propose <em>THGS</em>,
                            a novel method that extends talking head techniques to reconstruct expressive human avatars using 3D Gaussian Splatting (3DGS) from dynamic monocular videos.
                            After training on videos, <em>THGS</em> can animate speaker-specific human avatars with facial dynamics and hand gestures given speech audio and SMPL-X pose sequences.
                        </p>
                        <p>
                            <em>THGS</em> effectively overcomes the limitations of 3DGS-based human reconstruction methods in capturing expressive features like <strong>mouth movements, 
                            facial expressions, and hand gestures</strong> from dynamic monocular videos.
                        </p>
                        <p>
                            The key contributions of this paper are threefold. Firstly, we introduce a simple yet effective learnable expression blendshapes for head dynamics reconstruction, where avatar expression can be generated by linearly combining the static head model and expression blendshapes. 
                            Secondly, we employ a pose and expression coefficient refinement technique to optimize hand pose and facial expression accuracy on the fly by aligning 3D Gaussians with a human template mesh, which is crucial for human reconstruction pipeline. Thirdly, we use a <strong>Spatial Audio Attention Module (SAAM)</strong> for lip-synced mouth movement animation, which builds connections between speech audio and mouth Gaussian movements. 
                            Experimental results demonstrate that <em>THGS</em> achieves high-fidelity expressive 3D talking Gaussian human avatar animation at over 150 fps on a web-based rendering system.
                        </p>
                    </div>
                    <div id="image-container" class="content">
                        <img src="pipeline.png" class="dynamic-image" alt="Pipeline Image">
                        <p class="sub-description"><strong>Our Pipeline.</strong> Taking a monocular video as input, we learn a 3D Gaussian representation of talking human avatars. We initialize the 3D Gaussians on the SMPL-X vertices in the canonical space and drive the 3D Gaussians through LBS deformation to obtain posed Gaussians. For the LBS deformation module, we use pose θ and expression ψ as inputs. During training, θ, ψ, and skinning weights ω are optimized to achieve better body poses alignment. We also introduce a Learnable Expression Blendshapes to optimize facial dynamics reconstruction. Audio features are processed through the Spatial Audio Attention Module (SAAM), predicting mouth Gaussians deformation. We combine mouth Gaussians deformation with the posed Gaussians to produce the final Gaussians, P = {u+∆u,r+∆r,s+∆s,η,f}. Finally, a 3DGS rasterizer renders images based on camera poses.</p>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <section class="section" id="video1">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-four-fifths centered-content">
                    <video controls class="dynamic-video">
                        <source src="3Dvideos.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <p class="video-description">Our 3D video is here, rendering at a web-based visualizer(viser) at 285 FPS!</p>
                </div>
            </div>
        </div>
    </section>
    <section class="section" id="video2">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-four-fifths centered-content">
                    <video controls class="dynamic-video">
                        <source src="self_reenactment.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <p class="video-description">Our self-reenactment results. Given body pose and audio as input, our method can generate lip-synced talking human avatars with facial expressions and various hand gestures, outperforming human reconstruction SoTA method.</p>
                </div>
            </div>
        </div>
    </section>
    <section class="section" id="video3">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-four-fifths centered-content">
                    <video controls class="dynamic-video">
                        <source src="experssionControl.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <p class="video-description">Our expression control results, making smiling face, pouty lips, eyebrow movement, and blinking. Thanks to our LEB module, we can explicitly control over expression by linear interpolation of learnable expression blendshapes.</p>
                </div>
            </div>
        </div>
    </section>
    <section class="section" id="video4">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-four-fifths centered-content">
                    <video controls class="dynamic-video">
                        <source src="jointControl.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <p class="video-description">Our joint control results. We can explicitly control angles of wrist, elbow, shoulder, and head by adjusting rotation angles of SMPL-X.</p>
                </div>
            </div>
        </div>
    </section>
    <section class="section" id="video5">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-four-fifths centered-content">
                    <video controls class="dynamic-video">
                        <source src="LEB_com.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <p class="video-description">Our LEB ablation study. LEB significantly enhances facial dynamics by providing flexible basis functions that customize different expression bases to each identity.</p>
                </div>
            </div>
        </div>
    </section>

    <section class="section" id="video6">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-four-fifths centered-content">
                    <video controls class="dynamic-video">
                        <source src="talkinghead-6.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <p class="video-description">Comparison with 3D Talking Head Methods. We highlight the lip and neck artifacts in the previous methods. Noting that 3D talking head methods can only generate the head region. Whereas Our method, capable of generating a full-body 3D talking human, outperforms 3D talking head methods by maintaining robust neck driving under large head poses, minimizing lip artifacts, and ensuring superior temporal consistency in audio-driven scenarios.</p>
                </div>
            </div>
        </div>
    </section>
    <script>
        // JavaScript to dynamically set the image and video width to match the text width
        window.onload = function() {
            var textElements = document.querySelectorAll('.content p');
            var imageElements = document.querySelectorAll('.dynamic-image');
            var videoElements = document.querySelectorAll('.dynamic-video');
            textElements.forEach(function(textElement) {
                var textWidth = textElement.offsetWidth;
                imageElements.forEach(function(imageElement) {
                    imageElement.style.width = textWidth + 'px';
                });
                videoElements.forEach(function(videoElement) {
                    videoElement.style.width = textWidth + 'px';
                });
            });
        };
    </script>
</body>
</html>
